{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.253089Z",
     "start_time": "2020-01-30T20:32:54.498168Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from math import asin, cos, exp, pi, sin, sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from celluloid import Camera\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Write Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.274699Z",
     "start_time": "2020-01-30T20:33:00.255826Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimulatedAnnealing(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 T=1.0,\n",
    "                 anneal_rate=0.0003,\n",
    "                 min_temp=1e-5,\n",
    "                 anneal_every=100000):\n",
    "        defaults = dict(T=T,\n",
    "                        anneal_rate=anneal_rate,\n",
    "                        min_temp=min_temp,\n",
    "                        anneal_every=anneal_every,\n",
    "                        iteration=0)\n",
    "        super(SimulatedAnnealing, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is None:\n",
    "            raise Exception(\"loss closure is required to do SA\")\n",
    "\n",
    "        loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # clone all of the params to keep in case we need to swap back\n",
    "            cloned_params = [p.clone() for p in group['params']]\n",
    "\n",
    "            for p in group['params']:\n",
    "                # anneal temperature every 'anneal_every' iteration\n",
    "                if group['iteration'] > 0 \\\n",
    "                   and group['iteration'] % group['anneal_every'] == 0:\n",
    "                    group['T'] = np.maximum(group['T'] * group['anneal_rate'],\n",
    "                                            group['min_temp'])\n",
    "\n",
    "                # normalize weights\n",
    "                p.data = p.data / torch.norm(p.data)\n",
    "\n",
    "                # create randomizer\n",
    "                rand_float = torch.FloatTensor\n",
    "\n",
    "                # sample new weights with mu=old weights\n",
    "                if len(list(p.data.shape)) == 2:\n",
    "                    sampling_normal = []\n",
    "                    for d in p.data:\n",
    "                        temp = []\n",
    "                        for i in d:\n",
    "                            temp.append(rand_float(1).normal_(i, 2))\n",
    "                        sampling_normal.append(temp)\n",
    "                else:\n",
    "                    sampling_normal = []\n",
    "                    for d in p.data:\n",
    "                        sampling_normal.append(rand_float(1).normal_(i, .5))\n",
    "\n",
    "                p.data = torch.FloatTensor(sampling_normal)\n",
    "                group['iteration'] += 1\n",
    "\n",
    "            # re-evaluate the loss function with the perturbed params\n",
    "            # if we didn't accept the new params swap back and return\n",
    "            new_loss = closure()\n",
    "            final_loss, is_accepted = self.anneal(loss, new_loss, group['T'])\n",
    "            if not is_accepted:\n",
    "                for p, pbkp in zip(group['params'], cloned_params):\n",
    "                    p.data = pbkp.data\n",
    "\n",
    "            return final_loss\n",
    "\n",
    "    def anneal(self, loss, new_loss, temp):\n",
    "        '''returns loss, is_new_loss'''\n",
    "        def acceptance_prob(old, new, temp):\n",
    "            return torch.exp((old - new) / temp)\n",
    "\n",
    "        if new_loss.item() < loss.item():\n",
    "            return new_loss, True\n",
    "        else:\n",
    "            ap = acceptance_prob(loss, new_loss, temp)\n",
    "            u = np.random.rand()\n",
    "            if ap.item() >= u:\n",
    "                return new_loss, True\n",
    "\n",
    "            return loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Initialize parameters and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.295722Z",
     "start_time": "2020-01-30T20:33:00.279650Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "#loading iris data from sklearn\n",
    "iris = load_iris()\n",
    "x_data = iris.data\n",
    "y_data = iris.target\n",
    "\n",
    "#numpy to pytorch variable\n",
    "x_data = Variable(torch.from_numpy(x_data))\n",
    "y_data = Variable(torch.from_numpy(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.309809Z",
     "start_time": "2020-01-30T20:33:00.298289Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(4, 100)\n",
    "        self.l2 = torch.nn.Linear(100, 100)\n",
    "        self.l3 = torch.nn.Linear(100, 3)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.relu(self.l1(x))\n",
    "        out2 = self.relu(self.l2(out1))\n",
    "        out3 = self.l3(out2)\n",
    "        y_pred = self.softmax(out3)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Test Model with Default Optimizer - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.966256Z",
     "start_time": "2020-01-30T20:33:00.312708Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "permutation = torch.randperm(x_data.size()[0])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, x_data.size()[0], batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_x, batch_y = x_data[indices], y_data[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_val = model(batch_x.float())\n",
    "        loss = criterion(y_pred_val, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            y_data_1 = batch_y.detach().numpy()\n",
    "            y_pred_val = y_pred_val.detach().numpy()\n",
    "            y_pred = [np.argmax(y) for y in y_pred_val]\n",
    "            accuracy = accuracy_score(y_data_1, y_pred)\n",
    "            print(\n",
    "                f'Epoch: {str(epoch)}, batch: {str(i)}, loss: {loss}, accuracy: {accuracy}'\n",
    "            )\n",
    "\n",
    "y_pred = model(x_data.float())\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_data_1 = y_data.detach().numpy()\n",
    "\n",
    "y_pred = [np.argmax(y) for y in y_pred]\n",
    "print(\"accuracy: \" + str(accuracy_score(y_data_1, y_pred)))\n",
    "\n",
    "y_pred = model(x_data.float())\n",
    "loss = loss_fun(y_pred, y_data)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Test Model with Custom Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.976049Z",
     "start_time": "2020-01-30T20:32:54.746Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "loss_fun = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "\n",
    "# Change parameters of optimizer\n",
    "optimizer = SimulatedAnnealing(model.parameters(),\n",
    "                               T=1.0,\n",
    "                               anneal_rate=0.0001,\n",
    "                               min_temp=1e-8,\n",
    "                               anneal_every=1000)\n",
    "\n",
    "permutation = torch.randperm(x_data.size()[0])\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for i in range(0, x_data.size()[0], batch_size):\n",
    "\n",
    "        # function for optimizer\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x.float())\n",
    "            loss = loss_fun(output, batch_y)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_x, batch_y = x_data[indices], y_data[indices]\n",
    "        y_pred_val = model(batch_x.float())\n",
    "        loss = optimizer.step(closure)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            # calculates accuracy for epochs\n",
    "            y_data_1 = batch_y.detach().numpy()\n",
    "            y_pred_val = y_pred_val.detach().numpy()\n",
    "            y_pred = [np.argmax(y) for y in y_pred_val]\n",
    "            accuracy = accuracy_score(y_data_1, y_pred)\n",
    "            print(\n",
    "                f'Epoch: {str(epoch)}, batch: {str(i)}, loss: {loss}, accuracy: {accuracy}'\n",
    "            )\n",
    "\n",
    "y_pred = model(x_data.float())\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_data_1 = y_data.detach().numpy()\n",
    "\n",
    "y_pred = [np.argmax(y) for y in y_pred]\n",
    "print(\"accuracy: \" + str(accuracy_score(y_data_1, y_pred)))\n",
    "\n",
    "y_pred = model(x_data.float())\n",
    "loss = loss_fun(y_pred, y_data)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Prepare data for further algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.978044Z",
     "start_time": "2020-01-30T20:32:54.838Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('city.csv')\n",
    "\n",
    "population = []\n",
    "for num in df['population']:\n",
    "    try:\n",
    "        population.append(int(num))\n",
    "    except ValueError:\n",
    "        num = num[:-3]\n",
    "        population.append(int(num))\n",
    "\n",
    "df['population'] = population\n",
    "\n",
    "df = df.sort_values(by=['population'], ascending=False, ignore_index=True)[:30]\n",
    "df = df.drop([\n",
    "    'postal_code', 'country', 'federal_district', 'region_type', 'region',\n",
    "    'area_type', 'area', 'city_type', 'city', 'settlement_type', 'settlement',\n",
    "    'kladr_id', 'fias_id', 'capital_marker', 'okato', 'oktmo', 'tax_office',\n",
    "    'timezone', 'population', 'foundation_year', 'fias_level'\n",
    "],\n",
    "             axis=1)\n",
    "\n",
    "df['address'] = [ad.split(' ')[-1] for ad in df['address']]\n",
    "df = df.set_index('address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Helping functions to calculate distance between two cities and in the whole route using Haversine formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.980187Z",
     "start_time": "2020-01-30T20:32:54.884Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(city1: pd.Series, city2: pd.Series):\n",
    "    p = pi / 180\n",
    "\n",
    "    lat1, lon1 = city1.geo_lat * p, city1.geo_lon * p\n",
    "    lat2, lon2 = city2.geo_lat * p, city2.geo_lon * p\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    r = 6371\n",
    "\n",
    "    h = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    return 2 * r * asin(sqrt(h))\n",
    "\n",
    "def get_whole_distance(tour, df):\n",
    "    tour_distance = 0\n",
    "\n",
    "    for i in range(len(tour)):\n",
    "        from_city = tour[i]\n",
    "\n",
    "        if i + 1 < len(tour):\n",
    "            destination_city = tour[i + 1]\n",
    "        else:\n",
    "            destination_city = tour[0]\n",
    "\n",
    "        tour_distance += distance(df.loc[from_city],\n",
    "                                  df.loc[destination_city])\n",
    "\n",
    "    return tour_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Simulated Annealing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.982781Z",
     "start_time": "2020-01-30T20:32:54.931Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulated_annealing(df, anneal_every=100, anneal_rate=.01):\n",
    "    # generates initial tour randomly\n",
    "    def generate_random_individual(destination_cities):\n",
    "        tour = []\n",
    "        for city in destination_cities:\n",
    "            tour.append(city)\n",
    "        # re-order tour\n",
    "        random.shuffle(tour)\n",
    "        return tour\n",
    "\n",
    "    # return acceptance probability\n",
    "    def acceptance_probability(energy, new_energy, temperature):\n",
    "        return exp(\n",
    "            (energy - new_energy) / temperature) if energy <= new_energy else 1\n",
    "\n",
    "    # initial params\n",
    "    temp = 10000\n",
    "    iteration = 0\n",
    "\n",
    "    # helping list\n",
    "    destination_cities = list(df.index)\n",
    " \n",
    "    # generate x0\n",
    "    tours = []\n",
    "    tour = generate_random_individual(destination_cities)\n",
    "    tours.append(tour)\n",
    "\n",
    "    print('initial solution distance: ', get_whole_distance(tour, df))\n",
    "\n",
    "    # will save best tour\n",
    "    best_tour = tour.copy()\n",
    "    while temp > 1:\n",
    "        new_solution = tour.copy()\n",
    "\n",
    "        tour_pos1 = int(len(new_solution) * np.random.rand())\n",
    "        tour_pos2 = int(len(new_solution) * np.random.rand())\n",
    "\n",
    "        city_swap1 = new_solution[tour_pos1]\n",
    "        city_swap2 = new_solution[tour_pos2]\n",
    "\n",
    "        new_solution[tour_pos2] = city_swap1\n",
    "        new_solution[tour_pos1] = city_swap2\n",
    "\n",
    "        current_energy = get_whole_distance(tour, df)\n",
    "        new_energy = get_whole_distance(new_solution, df)\n",
    "\n",
    "        if acceptance_probability(current_energy, new_energy,\n",
    "                                  temp) >= np.random.rand():\n",
    "            tour = new_solution.copy()\n",
    "\n",
    "        if get_whole_distance(tour, df) < get_whole_distance(best_tour, df):\n",
    "            best_tour = tour.copy()\n",
    "\n",
    "        if iteration > 0 \\\n",
    "                   and iteration % anneal_every == 0:\n",
    "            temp *= anneal_rate\n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            print('iteration â„–', iteration, '| temp = ', temp,\n",
    "                  '| current distance: ', get_whole_distance(tour, df))\n",
    "            \n",
    "        tours.append(tour)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    print('final solution distance: ', get_whole_distance(best_tour, df))\n",
    "    tours.append(best_tour)\n",
    "    return best_tour, tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.984898Z",
     "start_time": "2020-01-30T20:32:54.933Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tour, tours = simulated_annealing(df, anneal_every=1000, anneal_rate=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.986974Z",
     "start_time": "2020-01-30T20:32:54.980Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)\n",
    "\n",
    "for i in range(len(tours)):\n",
    "    x = [df.loc[city]['geo_lon'] for city in tours[i]]\n",
    "    y = [df.loc[city]['geo_lat'] for city in tours[i]]\n",
    "    plt.plot(x, y, '-o', color='blue')\n",
    "    for j, city in enumerate(tours[i]):\n",
    "        plt.annotate(city, (x[j], y[j]))\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('tour_0.1.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:33:00.988896Z",
     "start_time": "2020-01-30T20:32:54.982Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [df.loc[city]['geo_lon'] for city in tour]\n",
    "y = [df.loc[city]['geo_lat'] for city in tour]\n",
    "plt.plot(x, y, '-o', color='blue')\n",
    "for j, city in enumerate(tour):\n",
    "    plt.annotate(city, (x[j], y[j]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
